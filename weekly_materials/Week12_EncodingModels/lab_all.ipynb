{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import neurods as nds\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel\n",
    "import cortex\n",
    "# Configure defaults for plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.aspect'] = 'auto'\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "%matplotlib inline\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are functions we implemented in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "def OLS(X,Y):\n",
    "    return np.dot(inv(np.dot(X.T,X)),np.dot(X.T,Y))\n",
    "\n",
    "def compute_correlation(matrix_1, matrix_2):\n",
    "    matrix_1_norm  = zscore(matrix_1)\n",
    "    matrix_2_norm  = zscore(matrix_2)\n",
    "    corr = np.mean(matrix_1_norm*matrix_2_norm, axis = 0)\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Stimuli\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use freely available data from the Mitchell 2008 science paper: https://www.cs.cmu.edu/afs/cs/project/theo-73/www/science2008/data.html\n",
    "\n",
    "The experiment actually consist in subjects looking at words/line drawings that are presented in isolation:\n",
    "\n",
    "<img src=\"figures/science.png\" style=\"height: 300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading data:\n",
    "basedir = os.path.join(nds.io.data_list['fmri'],'word_picture')\n",
    "name = os.path.join(basedir,'subject_1.nii.gz')\n",
    "volumes = nibabel.load(name)\n",
    "data = volumes.get_data()\n",
    "print(data.shape)\n",
    "data = data.T\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading mask\n",
    "name = os.path.join(basedir,'subject_1_mask.nii')\n",
    "volume = nibabel.load(name)\n",
    "mask = volume.get_data()\n",
    "print(mask.shape)\n",
    "mask = mask.T\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# flatten data to a 2D matrix\n",
    "data = data[:,mask>0]\n",
    "print(data.shape)\n",
    "\n",
    "# zscore the data\n",
    "data = zscore(data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this package allows us to work with matlab data, which we need here to load the variables\n",
    "import scipy.io as sio\n",
    "\n",
    "# here we load the 60 words that comprise our stimuli\n",
    "words = sio.loadmat(os.path.join(basedir,'words.mat'))\n",
    "\n",
    "words = [s[0][0] for s in words['words']]\n",
    "\n",
    "print(\"Here are the stimulus words:\\n\")\n",
    "print (\" - \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, a stimulus was presented every 10 seconds, and the activity between 4 and 8 seconds after onset was averaged, resulting in one brain image for every stimulus presentation. Each stimulus was repeated 6 times, and the repetitions of all the stimuli was averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_num = 38 # change the word number\n",
    "sample_image = np.zeros_like(mask)-2\n",
    "sample_image[mask>0] = data[word_num]\n",
    "h = cortex.mosaic(sample_image) # can try with different color map: e.g. h = mosaic(image , cmap= cm.hot)\n",
    "plt.title(words[word_num],size=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset already accounts for the delay of the hemodynamic response, and therefore we should not be convolving our design matrix. We will see here how to contruct a design matrix appropriate for such an experiment.\n",
    "\n",
    "How can we represent the activity for items that do not belong into clear conditions? \n",
    "\n",
    "We could try to make each word be a condition. Ending up with 60 conditions. We see each word only once. How would that help us? We would be able to compute a contrast map between \"horse\" and \"table\", but that would not tell us much about why these differences occur, as \"horse\" and \"table\" vary in many ways. Also, learning a response per word will not allow us to know what the activity will be like for new words, such as \"goat\", \"pen\" etc.\n",
    "\n",
    "However, we know that new words have some features in common with our set of objects. What if we could learn the responses to specific properties of words (e.g. whether or not they are animate, whether or not they are edible etc...). Then we predict the activity of a novel word as a combination of the activities associated with its properties. For example, we can learn how the brain responds to objects that are manmade, inanimate, made of wood and that are used as tools, and we can estimate the brain response of \"pen\" as the combination of these responses.\n",
    "\n",
    "We will do all this in the multivariate regression framework we have used in the last labs. \n",
    "\n",
    "First, we need an annotation of the properties of these words. From looking at the list of words, it's clear that there are many properties that different sets of them share.\n",
    "\n",
    "We have access to a set of 218 questions for which every word has been labeled by multiple users on amazon mechanical Turk (Sudre et al., Neuroimage, 2012). These question were designed to represent the semantic properties of these objects. Additionally, 11 features describing the visual properties of the line drawings are also provided.\n",
    "\n",
    "The scale of the features is 1-5 with 1 being a 100% no and 5 being 100% yes.\n",
    "\n",
    "Try changing feature_i below. Try to see the different features, as well as the features 218-229 as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_data = sio.loadmat(os.path.join(basedir,'features.mat'))\n",
    "\n",
    "feature_names = feature_data['feature_names']\n",
    "features = feature_data['features']\n",
    "print(\"We have {0} features that describe the stimulus.\\n\".format(len(feature_names)))\n",
    "#print feature_names\n",
    "\n",
    "print(\"The features matrix therefore has {0} rows and {1}.\\n\".format(len(words),len(feature_names)))\n",
    "\n",
    "\n",
    "feature_i = 10\n",
    "print(\"FEATURE NUMBER {0}\".format(feature_i))\n",
    "print(feature_names[feature_i][0][0])\n",
    "for i in range(15):\n",
    "    print(words[i], features[i,feature_i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Features 1 to 218\\n\")\n",
    "for i in range(15):\n",
    "    print (feature_names[i][0][0])\n",
    "print (\"...\")\n",
    "\n",
    "print (\"\\n\\nFeatures 219 to 229\\n\")\n",
    "for i in range(218,229):\n",
    "    print (feature_names[i][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take out two subsets of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_1 = features[:,:12]\n",
    "features_2 = features[:,218:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING A PREDICTIVE MODEL\n",
    "\n",
    "### IT IS VERY IMPORTANT NOT TO USE TEST DATA IN TRAINING!!\n",
    "\n",
    "To judge if a model has learned to predict brain activity outside, we need test it on data it has not seen in training. \n",
    "\n",
    "Imagine you have a small dataset with voxel responses to features, and some of the voxels have some noise that is correlated to one of the features. The probability of such an event becomes smaller as the dataset size increases, but at low sample sizes there is a good chance of finding spurious correlations. Such a correlation actually allows you to build a model that predicts brain activity from the features, but only in that dataset, since the noise is independent of the data and will not repeat in the same way in other datasets. However, for the voxels that show a real and strong enough response to the features, you will be able to learn a model that predicts brain activity from the features, and that model should generalize to new data.\n",
    "\n",
    "This is why we always test a model on held out data that was not used in training. This allows us to judge whether the model is really predicting neural activity and not just fitted to noise in the sample.\n",
    "\n",
    "Here we separate for you the words into a test and a train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Test_index = [0,1,2,3,4,6,7,8,10,13,20,23]\n",
    "Train_index = list(set(range(60)) - set(Test_index))\n",
    "\n",
    "Train_X_1 = np.nan_to_num(zscore(features_1[Train_index,:]))\n",
    "Train_X_2 = zscore(features_2[Train_index,:])\n",
    "Train_Y = zscore(data[Train_index,:])\n",
    "\n",
    "Test_X_1 = np.nan_to_num(zscore(features_1[Test_index,:]))\n",
    "Test_X_2 = zscore(features_2[Test_index,:])\n",
    "Test_Y = zscore(data[Test_index,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight estimation and data prediction\n",
    "\n",
    "We want to learn a function that predicts the activity for any word in terms of its features. \n",
    "\n",
    "\n",
    "#### Feature set 1\n",
    "- Use the OLS function to estimate the brain response to the features in features_1 for every voxel.\n",
    "- Use the estimated weights to predict the activity for the held-out words, using Test_X_1.\n",
    "- Use the compute_correlation function to compute the correlation of your predicted activity and the real activity Test_Y\n",
    "- Plot a flatmap of the prediction performance. Which regions are well predicted, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cor = compute_correlation(Test_Y, Pred_Y)\n",
    "# vol = cortex.Volume(cor, 'MNI', 'atlas336', mask=mask, vmin=0, vmax=0.6, cmap='viridis')\n",
    "# fig = cortex.quickflat.make_figure(vol, height=500)\n",
    "# plt.title(\"prediction performance with features set 1\", fontsize=20)\n",
    "\n",
    "### STUDENT ANSWER\n",
    "weights = OLS(Train_X_1, Train_Y)\n",
    "Pred_Y = np.dot(Test_X_1, weights)\n",
    "corr = compute_correlation(Test_Y, Pred_Y)\n",
    "vol = cortex.Volume(corr, 'MNI', 'atlas336', mask=mask, vmin=0, vmax=0.6)\n",
    "fig = cortex.quickflat.make_figure(vol)\n",
    "plt.title(\"prediction performance with features set 1\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature set 2\n",
    "Repeat the above for feature set 2:\n",
    "- Use the OLS function to estimate the brain response to the features in features_2 for every voxel.\n",
    "- Use the estimated weights to predict the activity for the held-out words, using Test_X_2.\n",
    "- Use the compute_correlation function to compute the correlation of your predicted activity and the real activity Test_Y\n",
    "- Plot a flatmap of the prediction performance. Which regions are well predicted, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cor = compute_correlation(Test_Y, Pred_Y)\n",
    "# vol = cortex.Volume(cor, 'MNI', 'atlas336', mask=mask, vmin=0, vmax=0.6, cmap='viridis')\n",
    "# fig = cortex.quickflat.make_figure(vol, height=500)\n",
    "# plt.title(\"prediction performance with features set 2\", fontsize=20)\n",
    "\n",
    "### STUDENT ANSWER\n",
    "weights = OLS(Train_X_2, Train_Y)\n",
    "Pred_Y = np.dot(Test_X_2, weights)\n",
    "corr = compute_correlation(Test_Y, Pred_Y)\n",
    "vol = cortex.Volume(corr, 'MNI', 'atlas336', mask=mask, vmin=0, vmax=0.6)\n",
    "fig = cortex.quickflat.make_figure(vol)\n",
    "plt.title(\"prediction performance with features set 2\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that features_1 has information about the object categories, and features_2 has information about the stimulus visual feature, how can you interpret the difference in the two maps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "# Different parts of the brain are predicted by the different feature sets.\n",
    "# The visual features predict more posterior parts of the brain including the primary visual cortex.\n",
    "# The semantic features predict more anterior parts of the brain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
