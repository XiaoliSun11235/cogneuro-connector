{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Frequency analysis in ECoG\n",
    "\n",
    "This week's lab will explore the frequency representation of ECoG. We'll calculate the full time-frequency representation of an ECoG signal, and then focus on a particular frequency band of interest.\n",
    "\n",
    "We'll now be going back into the world of ECoG, to see what differences we find. We'll use the same \"consonant/dissonant\" dataset seen in previous weeks. Perhaps by investigating the time-frequency representation of the signal we will uncover things that we couldn't see before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import datascience as ds\n",
    "import neurods as nds\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load in the raw data from the consonant/dissonant task\n",
    "* Also load the image of the brain and layout of ECoG channels\n",
    "* Finally, load in the event timing information for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_ecog = '../../data/ecog/chords_task/'\n",
    "data_path = path_ecog + 'ecog_resamp-raw.fif'\n",
    "event_path = path_ecog + 'meta_time.csv'\n",
    "\n",
    "### STUDENT ANSWER\n",
    "raw = mne.io.Raw(data_path, preload=True)\n",
    "events = ds.Table.read_table(event_path, index_col=0)\n",
    "im = plt.imread(path_ecog + 'brain.png')\n",
    "lt = mne.channels.read_layout('brain', path_ecog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convert the events information into an MNE events object. Make sure to include all event types in the events object.\n",
    "* Also create an events dictionary that maps event types onto event ID numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "ev_ixs = events['start'] * raw.info['sfreq']\n",
    "ev_ixs = ev_ixs.astype(int)\n",
    "ev_types = np.where(events['type'] == 'consonant', 1, 2)\n",
    "ev_mne = np.vstack([ev_ixs, np.zeros_like(ev_ixs), ev_types]).T\n",
    "ev_dict = dict(consonant=1, dissonant=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using this events array and our events dictionary, convert the `Raw` data to an `Epochs` object\n",
    "* Plot the global field power for each event type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "epochs = mne.Epochs(raw, ev_mne, event_id=ev_dict,\n",
    "                    tmin=-.5, tmax=.5, preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "# First, plot the global power of each stim type (for all channels)\n",
    "f, ax = plt.subplots()\n",
    "for dtype in ['dissonant', 'consonant']:\n",
    "    i_data = epochs[dtype]._data.mean(axis=0) ** 2\n",
    "    i_data = i_data.mean(axis=0)\n",
    "    ax.plot(epochs.times, i_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the plot we generated last week. It looks like the differences between conditions is relatively small, so let's see if something else comes out of a time-frequency analysis.\n",
    "\n",
    "* Convert the `Epochs` object into a time-frequency representation.\n",
    "  * Use frequencies from 4 to 140 spaced 2Hz apart\n",
    "  * Accomplish this with the neurods `tfr_morlet` function.\n",
    "  * Print the shape of the output for each condition. What does each axis represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "freqs = np.arange(4, 140, 2)\n",
    "tfrs = {'consonant': [], 'dissonant': []}\n",
    "for dtype in ['consonant', 'dissonant']:\n",
    "    tfrs[dtype] = nds.tfr.tfr_morlet(epochs[dtype]._data, epochs.info['sfreq'],\n",
    "                                     freqs, average=True, use_fft=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For each condition:\n",
    "  * Convert the output of `tfr_morlet` into an `AverageTFR` object. This will let you plot it. Use the docstring from `AverageTFR` to figure out how to create an object from data.\n",
    "  * Plot a topographic map of the TFR for each electrode. Use the `plot_tfr_topo` function in neurods.\n",
    "  * Plot an image of the brain in the background. For this, use the `add_background_image` function in `mne.viz`.\n",
    "  * In the call to `plot_topo`, use `mode='zscore', vmin=-5, vmax=5, cmap=plt.cm.coolwarm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "for condition in ['consonant', 'dissonant']:\n",
    "    avtfr = mne.time_frequency.AverageTFR(epochs.info, tfrs[condition],\n",
    "                                          epochs.times, freqs,\n",
    "                                          tfrs[condition].shape[0])\n",
    "    fig = nds.viz.plot_tfr_topo(avtfr, im, lt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, plot the difference (dissonant - consonant) for each TFR. Convert this into an MNE `AverageTFR` object.\n",
    "* Plot the result on the brain as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "diff = tfrs['dissonant'] - tfrs['consonant']\n",
    "\n",
    "avtfr = mne.time_frequency.AverageTFR(epochs.info, diff,\n",
    "                                      epochs.times, freqs,\n",
    "                                      tfrs['consonant'].shape[0])\n",
    "_ = nds.viz.plot_tfr_topo(avtfr, im, lt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Do you see any differences between the two groups?\n",
    "* How would you test for a difference between the two?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting high-frequency activity\n",
    "It seems like there are two \"blobs\" of high-frequency activity in the bove plots. Moreover, these blobs are more localized to specific electrodes than the ERPs of raw activity we saw before. Let's take a look at this activity.\n",
    "\n",
    "* Extract the first Epoch from the consonant data. Insert it into a new variable.\n",
    "* Using a bank of morlet wavelets, filter the epoch with 5 equally-spaced frequencies from 70-140Hz.\n",
    "* Take the \"real\" component of the result (using `np.real`), and average across frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "freqs = np.linspace(70, 140, 5)\n",
    "one_epoch = epochs._data[30]\n",
    "tfr = mne.time_frequency.cwt_morlet(one_epoch, epochs.info['sfreq'], freqs)\n",
    "tfr = np.real(tfr).mean(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the result in a single plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "f, ax = plt.subplots()\n",
    "_ = ax.plot(epochs.times, tfr.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What do you notice about the edges of the plot?\n",
    "* Try the same process with a few other epochs, see if the edges look consistent.\n",
    "\n",
    "The peaks that you see at the corners are called \"edge artifacts\". In order to get around them, we need to remove timepoints near the edges of our data (e.g., at the beginning and end). As such, it is generally better to extract the frequency amplitudes from the **`Raw`** data, and then convert it to epochs.\n",
    "\n",
    "* Using the `extract_amplitude` function in `neurods`, extract 5 linearly-spaced amplitudes from 70-150 Hz (define them manually or try using `np.linspace`). This function does the same thing that you did above, but it is more efficient with memory use.\n",
    "* Turn this `Raw` object into an `Epochs` object using the same event times that we used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "raw_amp = nds.tfr.extract_amplitude(raw, freqs)\n",
    "ep_amp = mne.Epochs(raw_amp, ev_mne, event_id=ev_dict,\n",
    "                    tmin=-.5, tmax=.5, preload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create an average response for each event type \n",
    "* Plot this average response (with the `.plot` method)\n",
    "* For make a list of the top 3 active channels that you determine by visual inspection (you can see the channel names by clicking on each trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "av_c = ep_amp['consonant'].average()\n",
    "av_d = ep_amp['dissonant'].average()\n",
    "\n",
    "_ = av_c.plot()\n",
    "_ = av_d.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "active_chs = ['ch_57', 'ch_49', 'ch_50']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, plot each one on the brain using the ECoG layout we loaded above. Remember we can do this with the `plot_topo` function, and don't forget to include an image of the brain as well (which we've also already loaded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "_ = av_c.plot_topo(lt, fig_background=im)\n",
    "_ = av_d.plot_topo(lt, fig_background=im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, plot the difference (dissonant - consonant) between the two on the brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER\n",
    "av_diff = av_d - av_c\n",
    "_ = av_diff.plot_topo(lt, fig_background=im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Does it look like there is a difference between the two?\n",
    "* Is it more or less localized than the signals that we've looked at so far?\n",
    "* Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### STUDENT ANSWER"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
